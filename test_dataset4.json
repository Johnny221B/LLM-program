[
    {
        "code": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport wandb\nimport time\n\nwandb.init(project=\"dcgan-cifar100&10\")\n\ndata_transform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ndataset = datasets.CIFAR100(root='./data', train=True, download=False, transform=data_transform)\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True)\n\nclass Gen(nn.Module):\n    def __init__(self):\n        super(Gen, self).__init__()\n        self.layer_stack = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        return self.layer_stack(z)\n\nclass Disc(nn.Module):\n    def __init__(self):\n        super(Disc, self).__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.layer_stack(x).view(-1)\n\ngenerator = Gen()\ndiscriminator = Disc()\n\ngenerator.apply(lambda m: m.__class__.__name__ == 'ConvTranspose2d' and nn.init.normal_(m.weight.data, 0.0, 0.02))\ndiscriminator.apply(lambda m: m.__class__.__name__ == 'Conv2d' and nn.init.normal_(m.weight.data, 0.0, 0.02))\n\noptimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngenerator.to(device)\ndiscriminator.to(device)\n\nos.makedirs('results', exist_ok=True)\n\nnum_epochs = 100\nlabel_real = 1.\nlabel_fake = 0.\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    for i, data in enumerate(data_loader, 0):\n        real_images = data[0].to(device)\n        batch_size = real_images.size(0)\n        real_labels = torch.full((batch_size,), label_real, dtype=torch.float, device=device)\n\n        discriminator.zero_grad()\n        real_output = discriminator(real_images)\n        err_real = nn.BCELoss()(real_output, real_labels)\n        err_real.backward()\n        real_score = real_output.mean().item()\n\n        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n        fake_images = generator(noise)\n        fake_labels = torch.full((batch_size,), label_fake, dtype=torch.float, device=device)\n        fake_output = discriminator(fake_images.detach())\n        err_fake = nn.BCELoss()(fake_output, fake_labels)\n        err_fake.backward()\n        fake_score = fake_output.mean().item()\n        optimizer_d.step()\n\n        generator.zero_grad()\n        output_g = discriminator(fake_images)\n        err_g = nn.BCELoss()(output_g, real_labels)\n        err_g.backward()\n        optimizer_g.step()\n\n        if i % 50 == 0:\n            print(f'[{epoch}/{num_epochs}][{i}/{len(data_loader)}] Loss_D: {err_real.item() + err_fake.item()} Loss_G: {err_g.item()} D(x): {real_score} D(G(z)): {fake_score}')\n\n    if epoch % 5 == 0:\n        vutils.save_image(real_images, f'results/real_samples_epoch_{epoch}.png', normalize=True)\n        vutils.save_image(fake_images.detach(), f'results/fake_samples_epoch_{epoch}.png', normalize=True)\n\nend_time = time.time()\nprint(f'Training finished. Total time: {end_time - start_time:.2f} seconds')\nwandb.finish()\n\ntorch.save(generator.state_dict(), 'generator.pth')\ntorch.save(discriminator.state_dict(), 'discriminator.pth')",
        "time": 1884
    },
    {
        "code": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport wandb\nimport time\n\nwandb.init(project=\"dcgan-cifar100&10\")\n\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform)\ndataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)\n\nnetG = Generator()\nnetD = Discriminator()\n\nnetG.apply(lambda m: m.__class__.__name__ == 'ConvTranspose2d' and nn.init.normal_(m.weight.data, 0.0, 0.02) or (m.__class__.__name__ == 'BatchNorm2d' and nn.init.normal_(m.weight.data, 1.0, 0.02) and nn.init.constant_(m.bias.data, 0)))\nnetD.apply(lambda m: m.__class__.__name__ == 'Conv2d' and nn.init.normal_(m.weight.data, 0.0, 0.02) or (m.__class__.__name__ == 'BatchNorm2d' and nn.init.normal_(m.weight.data, 1.0, 0.02) and nn.init.constant_(m.bias.data, 0)))\n\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnetG.to(device)\nnetD.to(device)\n\nos.makedirs('results', exist_ok=True)\n\nnum_epochs = 100\nreal_label = 1.\nfake_label = 0.\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        real_label_tensor = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        fake_label_tensor = torch.full((b_size,), fake_label, dtype=torch.float, device=device)\n\n        netD.zero_grad()\n        output = netD(real_cpu).view(-1)\n        errD_real = nn.BCELoss()(output, real_label_tensor)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        noise = torch.randn(b_size, 100, 1, 1, device=device)\n        fake = netG(noise)\n        output = netD(fake.detach()).view(-1)\n        errD_fake = nn.BCELoss()(output, fake_label_tensor)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        optimizerD.step()\n\n        netG.zero_grad()\n        output = netD(fake).view(-1)\n        errG = nn.BCELoss()(output, real_label_tensor)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        if i % 50 == 0:\n            wandb.log({\n                \"epoch\": epoch,\n                \"Loss_D\": errD_real.item() + errD_fake.item(),\n                \"Loss_G\": errG.item(),\n                \"D(x)\": D_x,\n                \"D(G(z1))\": D_G_z1,\n                \"D(G(z2))\": D_G_z2\n            })\n            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {errD_real.item() + errD_fake.item()} Loss_G: {errG.item()} D(x): {D_x} D(G(z)): {D_G_z1}/{D_G_z2}')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training finished. Total time: {:.2f} seconds'.format(training_time))\nwandb.finish()\n\ntorch.save(netG.state_dict(), './generator.pth')\ntorch.save(netD.state_dict(), './discriminator.pth')",
        "time": 1860
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport timm\nimport wandb\nimport time\n\nwandb.init(project=\"gpu-performance-benchmark\")\n\ndata_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=data_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=data_transforms)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nclass VisionTransformerWithDropout(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super(VisionTransformerWithDropout, self).__init__()\n        self.base_model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.base_model(x)\n        return self.dropout(x)\n\nmodel = VisionTransformerWithDropout('vit_base_patch16_224', 10)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % 200 == 199:\n            print(f'Batch {i + 1}, Loss: {running_loss / 200:.3f}')\n            running_loss = 0.0\n            wandb.log({\"batch_loss\": running_loss / 200})\n\ndef validate(model, loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_accuracy = 100 * correct / total\n    avg_val_loss = val_loss / len(loader)\n    print(f'Validation Loss: {avg_val_loss:.3f}, Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy})\n    return avg_val_loss, val_accuracy\n\nstart_time = time.time()\nnum_epochs = 70\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    train_one_epoch(model, train_loader, optimizer, criterion, device)\n    validate(model, test_loader, criterion, device)\n    scheduler.step()\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint(f'Training complete in {training_time:.2f} seconds')\n\nwandb.log({\"training_time\": training_time})\n\ntorch.save(model.state_dict(), './vit_cifar10_model_1.pth')\nwandb.finish()",
        "time": 30624
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport timm\nimport wandb\nimport time\n\nwandb.init(project=\"gpu-performance-benchmark\")\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, model_name, num_classes, dropout_rate=0.5):\n        super(VisionTransformer, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = self.dropout(x)\n        return x\n\ndef train_epoch(model, data_loader, criterion, optimizer, device):\n    model.train()\n    epoch_loss = 0\n    for batch_idx, (inputs, targets) in enumerate(data_loader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        if batch_idx % 200 == 199:\n            print(f'Batch {batch_idx + 1}, Loss: {epoch_loss / (batch_idx + 1):.3f}')\n            wandb.log({\"epoch_loss\": epoch_loss / (batch_idx + 1)})\n\ndef validate_epoch(model, data_loader, criterion, device):\n    model.eval()\n    validation_loss = 0\n    correct_predictions = 0\n    total_samples = 0\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            validation_loss += loss.item()\n            _, preds = torch.max(outputs, 1)\n            total_samples += targets.size(0)\n            correct_predictions += (preds == targets).sum().item()\n    validation_accuracy = 100 * correct_predictions / total_samples\n    avg_val_loss = validation_loss / len(data_loader)\n    print(f'Validation Loss: {avg_val_loss:.3f}, Accuracy: {validation_accuracy:.2f}%')\n    wandb.log({\"validation_loss\": avg_val_loss, \"validation_accuracy\": validation_accuracy})\n    return avg_val_loss, validation_accuracy\n\nmodel = VisionTransformer('vit_base_patch16_224', num_classes=10, dropout_rate=0.5)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nepochs = 70\nstart = time.time()\n\nfor epoch in range(epochs):\n    print(f'Starting Epoch {epoch + 1}/{epochs}')\n    train_epoch(model, train_loader, criterion, optimizer, device)\n    validate_epoch(model, test_loader, criterion, device)\n    scheduler.step()\n\nend = time.time()\nprint(f'Training completed in {end - start:.2f} seconds')\n\nwandb.log({\"total_training_time\": end - start})\n\ntorch.save(model.state_dict(), './vit_cifar10_model_2.pth')\nwandb.finish()",
        "time": 31180
    },
    {
        "code": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW\nimport wandb\nimport time\n\nwandb.init(project=\"sentiment analysis\")\n\ntrain_dataset = torch.load('data/train_dataset.pt')\ntest_dataset = torch.load('data/test_dataset.pt')\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\nbatch_size = 64\nlearning_rate = 3e-5\nepochs = 75\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train(model, dataloader, optimizer):\n    model.train()\n    total_loss = sum(loss.item() for loss in (\n        model(batch['input_ids'].to(device), \n              batch['attention_mask'].to(device), \n              labels=batch['labels'].to(device)\n             ).loss.backward() or optimizer.step() or optimizer.zero_grad() or model(batch['input_ids'], batch['attention_mask'], labels=batch['labels']).loss\n        for batch in dataloader))\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    total, correct = sum((batch['labels'].size(0), (model(batch['input_ids'].to(device), batch['attention_mask'].to(device)).logits.argmax(-1) == batch['labels'].to(device)).sum().item()) for batch in dataloader)\n    accuracy = correct / total\n    return accuracy, total_loss / len(dataloader)\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    train_loss = train(model, train_loader, optimizer)\n    val_accuracy, val_loss = evaluate(model, test_loader)\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}')\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_accuracy\": val_accuracy,\n        \"val_loss\": val_loss\n    })\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nwandb.finish()",
        "time": 48147
    },
    {
        "code": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW\nimport wandb\nimport time\n\nwandb.init(project=\"sentiment analysis\")\n\ntrain_dataset = torch.load('data/train_dataset.pt')\ntest_dataset = torch.load('data/test_dataset.pt')\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\nbatch_size = 64\nlearning_rate = 3e-5\nepochs = 75\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef process_batch(batch):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    labels = batch['labels'].to(device)\n    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n    return outputs.loss\n\ndef train(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0.0\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        loss = process_batch(batch)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    total_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    for batch in dataloader:\n        with torch.no_grad():\n            loss = process_batch(batch)\n            logits = model(batch['input_ids'].to(device), batch['attention_mask'].to(device)).logits\n            predictions = logits.argmax(-1)\n            total_correct += (predictions == batch['labels'].to(device)).sum().item()\n            total_samples += batch['labels'].size(0)\n            total_loss += loss.item()\n    accuracy = total_correct / total_samples\n    return accuracy, total_loss / len(dataloader)\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    train_loss = train(model, train_loader, optimizer)\n    val_accuracy, val_loss = evaluate(model, test_loader)\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}')\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_accuracy\": val_accuracy,\n        \"val_loss\": val_loss\n    })\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nwandb.finish()",
        "time": 47899
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport wandb\nimport time\n\nwandb.init(project=\"cifar100-resnet50\")\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 100)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train_one_epoch(model, data_loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    for batch_idx, (inputs, labels) in enumerate(data_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if batch_idx % 200 == 199:\n            avg_loss = running_loss / 200\n            print(f'Epoch [{batch_idx + 1}], Loss: {avg_loss:.3f}')\n            running_loss = 0.0\n            wandb.log({\"batch\": batch_idx + 1, \"loss\": avg_loss})\n\ndef validate(model, data_loader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    avg_val_loss = val_loss / len(data_loader)\n    val_accuracy = 100 * correct / total\n    print(f'Validation Loss: {avg_val_loss:.3f}, Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": avg_val_loss, \"val_accuracy\": val_accuracy})\n\nnum_epochs = 70\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    train_one_epoch(model, train_loader, optimizer, criterion, device)\n    validate(model, test_loader, criterion, device)\n    scheduler.step()\n\nend_time = time.time()\ntraining_duration = end_time - start_time\nprint(f'Training completed in {training_duration:.2f} seconds')\nwandb.log({\"training_time\": training_duration})\n\ntorch.save(model.state_dict(), './resnet50_cifar100_model_1.pth')\nwandb.finish()",
        "time": 8880
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport wandb\nimport time\n\nwandb.init(project=\"cifar100-resnet50\")\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntrain_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n\ntest_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)\n\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 100)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train_and_validate(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs):\n    start = time.time()\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0.0\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs, targets = inputs.to(device), targets.to(device)\n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            if i % 200 == 199:\n                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {train_loss / 200:.3f}')\n                wandb.log({\"epoch\": epoch + 1, \"batch\": i + 1, \"train_loss\": train_loss / 200})\n                train_loss = 0.0\n\n        scheduler.step()\n\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n\n        val_accuracy = 100 * correct / total\n        print(f'Epoch {epoch + 1}, Validation Loss: {val_loss / len(test_loader):.3f}, Accuracy: {val_accuracy:.2f}%')\n        wandb.log({\"epoch\": epoch + 1, \"val_loss\": val_loss / len(test_loader), \"val_accuracy\": val_accuracy})\n\n    end = time.time()\n    print(f'Training Time: {end - start:.2f} seconds')\n    wandb.log({\"training_time\": end - start})\n\ntrain_and_validate(model, train_loader, test_loader, criterion, optimizer, scheduler, device, num_epochs=70)\n\ntorch.save(model.state_dict(), './resnet50_cifar100_model_2.pth')\nwandb.finish()",
        "time": 8838
    }
]