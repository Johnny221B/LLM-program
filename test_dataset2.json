[
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport wandb\nimport time\n\nwandb.init(project=\"cifar100-resnet50\")\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 100)  \n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 50\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 200 == 199:\n            avg_loss = running_loss / 200\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n            running_loss = 0.0\n            wandb.log({\"epoch\": epoch + 1, \"batch\": i + 1, \"loss\": avg_loss})\n\n    scheduler.step() \n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= len(testloader)\n    val_accuracy = 100 * correct / total\n    print(f'Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n\nprint('Finished Training')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nPATH = './resnet50_cifar100.pth'\ntorch.save(model.state_dict(), PATH)\n\nwandb.finish()",
        "time": 6680.702
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport wandb\nimport time\n\nwandb.init(project=\"cifar100-resnet50\",name=\"train_resnet509\")\n\ntransform_train = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n])\n\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(model.fc.in_features, 100)  \n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.002)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 75\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 200 == 199:\n            avg_loss = running_loss / 200\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n            running_loss = 0.0\n            wandb.log({\"epoch\": epoch + 1, \"batch\": i + 1, \"loss\": avg_loss})\n\n    scheduler.step()  \n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= len(testloader)\n    val_accuracy = 100 * correct / total\n    print(f'Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n\nprint('Finished Training')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nPATH = './resnet50_cifar100.pth'\ntorch.save(model.state_dict(), PATH)\n\nwandb.finish()",
        "time": 9445
    },
    {
        "code": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW\nimport wandb\nimport time\n\nwandb.init(project=\"sentiment analysis\")\n\ntrain_dataset = torch.load('data/train_dataset.pt')\ntest_dataset = torch.load('data/test_dataset.pt')\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\nbatch_size = 64\nlearning_rate = 3e-5\nepochs = 60\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train(model, dataloader, optimizer):\n    model.train()\n    running_loss = 0.0\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % 10 == 0:\n            print(f\"Batch {i}, Loss: {loss.item()}\")\n    return running_loss / len(dataloader)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n            running_loss += loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n            if i % 100 == 0:\n                print(f\"Batch {i}, Evaluation Loss: {loss.item()}\")\n    accuracy = correct / total\n    return accuracy, running_loss / len(dataloader)\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}/{epochs}\")\n    train_loss = train(model, train_loader, optimizer)\n    val_accuracy, val_loss = evaluate(model, test_loader)\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}')\n    \n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_accuracy\": val_accuracy,\n        \"val_loss\": val_loss\n    })\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nwandb.finish()",
        "time": 39173
    },
    {
        "code": "import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, AdamW\nimport wandb\nimport time\n\nwandb.init(project=\"sentiment analysis\",name=\"run_bert9\")\n\ntrain_dataset = torch.load('data/train_dataset.pt')\ntest_dataset = torch.load('data/test_dataset.pt')\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\nbatch_size = 32\nlearning_rate = 3e-5\nepochs = 70\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\ndevice = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ndef train(model, dataloader, optimizer):\n    model.train()\n    running_loss = 0.0\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        if i % 10 == 0:  \n            print(f\"Batch {i}, Loss: {loss.item()}\")\n    return running_loss / len(dataloader)\n\ndef evaluate(model, dataloader):\n    model.eval()\n    correct = 0\n    total = 0\n    running_loss = 0.0\n    criterion = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        for i, batch in enumerate(dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(outputs.logits, labels)\n            running_loss += loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n            if i % 100 == 0:  \n                print(f\"Batch {i}, Evaluation Loss: {loss.item()}\")\n    accuracy = correct / total\n    return accuracy, running_loss / len(dataloader)\n\nstart_time = time.time()\n\nfor epoch in range(epochs):  \n    print(f\"Epoch {epoch+1}/{epochs}\")\n    train_loss = train(model, train_loader, optimizer)\n    val_accuracy, val_loss = evaluate(model, test_loader)\n    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}')\n    \n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"train_loss\": train_loss,\n        \"val_accuracy\": val_accuracy,\n        \"val_loss\": val_loss\n    })\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nwandb.finish()",
        "time": 45767
    },
    {
        "code": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport wandb\nimport time\n\nwandb.init(project=\"dcgan-cifar100&10\")\n\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform)\ndataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)\n\nnetG = Generator()\nnetD = Discriminator()\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnetG.to(device)\nnetD.to(device)\ncriterion.to(device)\n\nos.makedirs('results', exist_ok=True)\n\nnum_epochs = 50\nreal_label = 1.\nfake_label = 0.\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        output = netD(real_cpu)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        noise = torch.randn(b_size, 100, 1, 1, device=device)\n        fake = netG(noise)\n        label.fill_(fake_label)\n        output = netD(fake.detach())\n        errD_fake = criterion(output, label)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        netG.zero_grad()\n        label.fill_(real_label)\n        output = netD(fake)\n        errG = criterion(output, label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        if i % 50 == 0:\n            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {errD.item()} Loss_G: {errG.item()} D(x): {D_x} D(G(z)): {D_G_z1}/{D_G_z2}')\n\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"Loss_D\": errD.item(),\n        \"Loss_G\": errG.item(),\n        \"D(x)\": D_x,\n        \"D(G(z1))\": D_G_z1,\n        \"D(G(z2))\": D_G_z2\n    })\n\n    vutils.save_image(real_cpu, f'results/real_samples_epoch_{epoch}.png', normalize=True)\n    fake = netG(noise)\n    vutils.save_image(fake.detach(), f'results/fake_samples_epoch_{epoch}.png', normalize=True)\n\nprint('Training finished.')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\nwandb.log({\"training_time\": training_time})\ntorch.save(netG.state_dict(), './dcgan_generator.pth')\ntorch.save(netD.state_dict(), './dcgan_discriminator.pth')\nwandb.finish()",
        "time": 916.043
    },
    {
        "code": "import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport wandb\nimport time\n\nwandb.init(project=\"dcgan-cifar100&10\",name=\"train_gan9.py\")\n\ntransform = transforms.Compose([\n    transforms.Resize(64),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrainset = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform)\ndataloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.main = nn.Sequential(\n            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input).view(-1)\n\nnetG = Generator()\nnetD = Discriminator()\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n\ndevice = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\nnetG.to(device)\nnetD.to(device)\ncriterion.to(device)\n\nos.makedirs('results', exist_ok=True)\n\nnum_epochs = 85\nreal_label = 1.\nfake_label = 0.\n\nstart_time = time.time()\nfor epoch in range(num_epochs):\n    for i, data in enumerate(dataloader, 0):\n        netD.zero_grad()\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        output = netD(real_cpu)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        noise = torch.randn(b_size, 100, 1, 1, device=device)\n        fake = netG(noise)\n        label.fill_(fake_label)\n        output = netD(fake.detach())\n        errD_fake = criterion(output, label)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        netG.zero_grad()\n        label.fill_(real_label)\n        output = netD(fake)\n        errG = criterion(output, label)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n\n        if i % 50 == 0:\n            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {errD.item()} Loss_G: {errG.item()} D(x): {D_x} D(G(z)): {D_G_z1}/{D_G_z2}')\n\n    wandb.log({\n        \"epoch\": epoch + 1,\n        \"Loss_D\": errD.item(),\n        \"Loss_G\": errG.item(),\n        \"D(x)\": D_x,\n        \"D(G(z1))\": D_G_z1,\n        \"D(G(z2))\": D_G_z2\n    })\n\n    vutils.save_image(real_cpu, f'results/real_samples_epoch_{epoch}.png', normalize=True)\n    fake = netG(noise)\n    vutils.save_image(fake.detach(), f'results/fake_samples_epoch_{epoch}.png', normalize=True)\n\nprint('Training finished.')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\nwandb.log({\"training_time\": training_time})\n\ntorch.save(netG.state_dict(), './dcgan_generator.pth')\ntorch.save(netD.state_dict(), './dcgan_discriminator.pth')\n\nwandb.finish()",
        "time": 1572
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport timm\nimport wandb\nimport time\n\nwandb.init(project=\"gpu-performance-benchmark\")\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n\nclass VisionTransformerWithDropout(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super(VisionTransformerWithDropout, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = self.dropout(x)\n        return x\n\nmodel = VisionTransformerWithDropout('vit_base_patch16_224', 10)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 50  \nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 200 == 199:\n            avg_loss = running_loss / 200\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n            running_loss = 0.0\n            wandb.log({\"epoch\": epoch + 1, \"batch\": i + 1, \"loss\": avg_loss})\n\n    scheduler.step() \n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= len(testloader)\n    val_accuracy = 100 * correct / total\n    print(f'Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n\nprint('Finished Training')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nPATH = './vit_cifar10.pth'\ntorch.save(model.state_dict(), PATH)\n\nwandb.finish()",
        "time": 19304.295
    },
    {
        "code": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport timm\nimport wandb\nimport time\n\nwandb.init(project=\"gpu-performance-benchmark\",name=\"train_ViT9\")\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n\nclass VisionTransformerWithDropout(nn.Module):\n    def __init__(self, model_name, num_classes):\n        super(VisionTransformerWithDropout, self).__init__()\n        self.model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.model(x)\n        x = self.dropout(x)\n        return x\n\nmodel = VisionTransformerWithDropout('vit_base_patch16_224', 10)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.002)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\ndevice = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nnum_epochs = 75\nstart_time = time.time()\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 200 == 199:\n            avg_loss = running_loss / 200\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n            running_loss = 0.0\n            wandb.log({\"epoch\": epoch + 1, \"batch\": i + 1, \"loss\": avg_loss})\n\n\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss /= len(testloader)\n    val_accuracy = 100 * correct / total\n    print(f'Validation Loss: {val_loss:.3f}, Validation Accuracy: {val_accuracy:.2f}%')\n    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n\nprint('Finished Training')\n\nend_time = time.time()\ntraining_time = end_time - start_time\nprint('Training Time: {:.2f} seconds'.format(training_time))\n\nwandb.log({\"training_time\": training_time})\n\nPATH = './vit_cifar10.pth'\ntorch.save(model.state_dict(), PATH)\n\nwandb.finish()",
        "time": 28168
    }
]