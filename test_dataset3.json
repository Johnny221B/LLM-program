[
    {
        "code": "import os\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport torchvision.datasets as datasets\n\nimport torchvision.transforms as transforms\n\nimport torchvision.utils as vutils\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"dcgan-cifar100&10\",name=\"gan-style1\")\n\n \n\n# \u914d\u7f6e\u53c2\u6570\n\nconfig = {\n\n    \"image_size\": 64,\n\n    \"batch_size\": 128,\n\n    \"num_epochs\": 100,\n\n    \"learning_rate\": 0.0002,\n\n    \"beta1\": 0.5,\n\n    \"nz\": 100,  # Size of z latent vector (i.e. size of generator input)\n\n    \"ngf\": 64,  # Size of feature maps in generator\n\n    \"ndf\": 64,  # Size of feature maps in discriminator\n\n    \"device\": torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\"),\n\n    \"real_label\": 1.0,\n\n    \"fake_label\": 0.0,\n\n    \"results_dir\": \"results\"\n\n}\n\n \n\n# \u6570\u636e\u9884\u5904\u7406\u548c\u52a0\u8f7d\n\ndef get_dataloader(config):\n\n    transform = transforms.Compose([\n\n        transforms.Resize(config[\"image_size\"]),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\n    ])\n\n \n\n    dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n\n    return dataloader\n\n \n\n# \u5b9a\u4e49\u751f\u6210\u5668\u6a21\u578b\n\nclass Generator(nn.Module):\n\n    def __init__(self, config):\n\n        super(Generator, self).__init__()\n\n        self.main = nn.Sequential(\n\n            nn.ConvTranspose2d(config[\"nz\"], config[\"ngf\"] * 8, 4, 1, 0, bias=False),\n\n            nn.BatchNorm2d(config[\"ngf\"] * 8),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(config[\"ngf\"] * 8, config[\"ngf\"] * 4, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ngf\"] * 4),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(config[\"ngf\"] * 4, config[\"ngf\"] * 2, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ngf\"] * 2),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(config[\"ngf\"] * 2, config[\"ngf\"], 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ngf\"]),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(config[\"ngf\"], 3, 4, 2, 1, bias=False),\n\n            nn.Tanh()\n\n        )\n\n \n\n    def forward(self, x):\n\n        return self.main(x)\n\n \n\n# \u5b9a\u4e49\u5224\u522b\u5668\u6a21\u578b\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, config):\n\n        super(Discriminator, self).__init__()\n\n        self.main = nn.Sequential(\n\n            nn.Conv2d(3, config[\"ndf\"], 4, 2, 1, bias=False),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(config[\"ndf\"], config[\"ndf\"] * 2, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ndf\"] * 2),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(config[\"ndf\"] * 2, config[\"ndf\"] * 4, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ndf\"] * 4),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(config[\"ndf\"] * 4, config[\"ndf\"] * 8, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(config[\"ndf\"] * 8),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(config[\"ndf\"] * 8, 1, 4, 1, 0, bias=False),\n\n            nn.Sigmoid()\n\n        )\n\n \n\n    def forward(self, x):\n\n        return self.main(x).view(-1)\n\n \n\n# \u521d\u59cb\u5316\u6a21\u578b\u548c\u4f18\u5316\u5668\n\ndef initialize_models(config):\n\n    netG = Generator(config).to(config[\"device\"])\n\n    netD = Discriminator(config).to(config[\"device\"])\n\n \n\n    netG.apply(weights_init)\n\n    netD.apply(weights_init)\n\n \n\n    optimizerD = optim.Adam(netD.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n\n    optimizerG = optim.Adam(netG.parameters(), lr=config[\"learning_rate\"], betas=(config[\"beta1\"], 0.999))\n\n \n\n    return netG, netD, optimizerD, optimizerG\n\n \n\ndef weights_init(m):\n\n    classname = m.__class__.__name__\n\n    if classname.find('Conv') != -1:\n\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n\n    elif classname.find('BatchNorm') != -1:\n\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n\n        nn.init.constant_(m.bias.data, 0)\n\n \n\n# \u8bad\u7ec3\u8fc7\u7a0b\n\ndef train_dcgan(dataloader, netG, netD, optimizerD, optimizerG, config):\n\n    criterion = nn.BCELoss().to(config[\"device\"])\n\n    real_label = config[\"real_label\"]\n\n    fake_label = config[\"fake_label\"]\n\n \n\n    os.makedirs(config[\"results_dir\"], exist_ok=True)\n\n \n\n    start_time = time.time()\n\n   \n\n    for epoch in range(config[\"num_epochs\"]):\n\n        for i, data in enumerate(dataloader, 0):\n\n            # \u66f4\u65b0\u5224\u522b\u5668\n\n            netD.zero_grad()\n\n            real_cpu = data[0].to(config[\"device\"])\n\n            b_size = real_cpu.size(0)\n\n            label = torch.full((b_size,), real_label, dtype=torch.float, device=config[\"device\"])\n\n            output = netD(real_cpu)\n\n            errD_real = criterion(output, label)\n\n            errD_real.backward()\n\n            D_x = output.mean().item()\n\n \n\n            noise = torch.randn(b_size, config[\"nz\"], 1, 1, device=config[\"device\"])\n\n            fake = netG(noise)\n\n            label.fill_(fake_label)\n\n            output = netD(fake.detach())\n\n            errD_fake = criterion(output, label)\n\n            errD_fake.backward()\n\n            D_G_z1 = output.mean().item()\n\n            errD = errD_real + errD_fake\n\n            optimizerD.step()\n\n \n\n            # \u66f4\u65b0\u751f\u6210\u5668\n\n            netG.zero_grad()\n\n            label.fill_(real_label)\n\n            output = netD(fake)\n\n            errG = criterion(output, label)\n\n            errG.backward()\n\n            D_G_z2 = output.mean().item()\n\n            optimizerG.step()\n\n \n\n            if i % 50 == 0:\n\n                print(f'[{epoch + 1}/{config[\"num_epochs\"]}][{i}/{len(dataloader)}] Loss_D: {errD.item()} Loss_G: {errG.item()} D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f}/{D_G_z2:.4f}')\n\n \n\n        # \u4fdd\u5b58\u548c\u8bb0\u5f55\n\n        wandb.log({\n\n            \"epoch\": epoch + 1,\n\n            \"Loss_D\": errD.item(),\n\n            \"Loss_G\": errG.item(),\n\n            \"D(x)\": D_x,\n\n            \"D(G(z1))\": D_G_z1,\n\n            \"D(G(z2))\": D_G_z2\n\n        })\n\n \n\n        fake = netG(torch.randn(config[\"batch_size\"], config[\"nz\"], 1, 1, device=config[\"device\"]))\n\n        vutils.save_image(real_cpu, f'{config[\"results_dir\"]}/real_samples_epoch_{epoch + 1}.png', normalize=True)\n\n        vutils.save_image(fake.detach(), f'{config[\"results_dir\"]}/fake_samples_epoch_{epoch + 1}.png', normalize=True)\n\n \n\n    end_time = time.time()\n\n    training_time = end_time - start_time\n\n    print(f'Training Time: {training_time:.2f} seconds')\n\n \n\n    wandb.log({\"training_time\": training_time})\n\n    torch.save(netG.state_dict(), os.path.join(config[\"results_dir\"], 'dcgan_generator.pth'))\n\n    torch.save(netD.state_dict(), os.path.join(config[\"results_dir\"], 'dcgan_discriminator.pth'))\n\n    wandb.finish()\n\n \n\n# \u4e3b\u7a0b\u5e8f\n\ndef main():\n\n    dataloader = get_dataloader(config)\n\n    netG, netD, optimizerD, optimizerG = initialize_models(config)\n\n    train_dcgan(dataloader, netG, netD, optimizerD, optimizerG, config)\n\n \n\nif __name__ == \"__main__\":\n\n    main()",
        "time": 1884
    },
    {
        "code": "import os\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nimport torchvision.datasets as datasets\n\nimport torchvision.transforms as transforms\n\nimport torchvision.utils as vutils\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"dcgan-cifar100&10\",name=\"gan-style2\")\n\n \n\n# \u6570\u636e\u9884\u5904\u7406\u4e0e\u52a0\u8f7d\n\ndef get_dataloader(image_size=64, batch_size=128):\n\n    transform = transforms.Compose([\n\n        transforms.Resize(image_size),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n\n    ])\n\n    dataset = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform)\n\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    return dataloader\n\n \n\n# \u6a21\u578b\u5b9a\u4e49\n\nclass DCGAN(nn.Module):\n\n    def __init__(self, nz, ngf, ndf, nc):\n\n        super(DCGAN, self).__init__()\n\n        self.generator = self.build_generator(nz, ngf, nc)\n\n        self.discriminator = self.build_discriminator(ndf, nc)\n\n \n\n    def build_generator(self, nz, ngf, nc):\n\n        return nn.Sequential(\n\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n\n            nn.BatchNorm2d(ngf * 8),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ngf * 4),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ngf * 2),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ngf),\n\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n\n            nn.Tanh()\n\n        )\n\n \n\n    def build_discriminator(self, ndf, nc):\n\n        return nn.Sequential(\n\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ndf * 2),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ndf * 4),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n\n            nn.BatchNorm2d(ndf * 8),\n\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n\n            nn.Sigmoid()\n\n        )\n\n \n\n    def initialize_weights(self):\n\n        for m in self.modules():\n\n            classname = m.__class__.__name__\n\n            if classname.find('Conv') != -1:\n\n                nn.init.normal_(m.weight.data, 0.0, 0.02)\n\n            elif classname.find('BatchNorm') != -1:\n\n                nn.init.normal_(m.weight.data, 1.0, 0.02)\n\n                nn.init.constant_(m.bias.data, 0)\n\n \n\n# \u6a21\u578b\u8bad\u7ec3\n\ndef train_dcgan(model, dataloader, num_epochs=100, lr=0.0002, beta1=0.5, device=\"cuda:5\"):\n\n    criterion = nn.BCELoss().to(device)\n\n    optimizerD = optim.Adam(model.discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n    optimizerG = optim.Adam(model.generator.parameters(), lr=lr, betas=(beta1, 0.999))\n\n \n\n    real_label = 1.0\n\n    fake_label = 0.0\n\n \n\n    os.makedirs('results', exist_ok=True)\n\n    fixed_noise = torch.randn(64, model.generator[0].in_channels, 1, 1, device=device)\n\n   \n\n    model.to(device)\n\n    model.initialize_weights()\n\n \n\n    start_time = time.time()\n\n \n\n    for epoch in range(num_epochs):\n\n        for i, data in enumerate(dataloader, 0):\n\n            # \u66f4\u65b0\u5224\u522b\u5668\n\n            model.discriminator.zero_grad()\n\n            real_data = data[0].to(device)\n\n            batch_size = real_data.size(0)\n\n            label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n\n            output = model.discriminator(real_data).view(-1)\n\n            lossD_real = criterion(output, label)\n\n            lossD_real.backward()\n\n            D_x = output.mean().item()\n\n \n\n            noise = torch.randn(batch_size, model.generator[0].in_channels, 1, 1, device=device)\n\n            fake_data = model.generator(noise)\n\n            label.fill_(fake_label)\n\n            output = model.discriminator(fake_data.detach()).view(-1)\n\n            lossD_fake = criterion(output, label)\n\n            lossD_fake.backward()\n\n            D_G_z1 = output.mean().item()\n\n            optimizerD.step()\n\n \n\n            # \u66f4\u65b0\u751f\u6210\u5668\n\n            model.generator.zero_grad()\n\n            label.fill_(real_label)\n\n            output = model.discriminator(fake_data).view(-1)\n\n            lossG = criterion(output, label)\n\n            lossG.backward()\n\n            D_G_z2 = output.mean().item()\n\n            optimizerG.step()\n\n \n\n            if i % 50 == 0:\n\n                print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {lossD_real.item() + lossD_fake.item()} Loss_G: {lossG.item()} D(x): {D_x} D(G(z)): {D_G_z1}/{D_G_z2}')\n\n \n\n        wandb.log({\n\n            \"epoch\": epoch + 1,\n\n            \"Loss_D\": lossD_real.item() + lossD_fake.item(),\n\n            \"Loss_G\": lossG.item(),\n\n            \"D(x)\": D_x,\n\n            \"D(G(z1))\": D_G_z1,\n\n            \"D(G(z2))\": D_G_z2\n\n        })\n\n \n\n        vutils.save_image(real_data, f'results/real_samples_epoch_{epoch}.png', normalize=True)\n\n        fake = model.generator(fixed_noise)\n\n        vutils.save_image(fake.detach(), f'results/fake_samples_epoch_{epoch}.png', normalize=True)\n\n \n\n    print(f'Training finished. Total Time: {time.time() - start_time:.2f} seconds')\n\n    wandb.log({\"training_time\": time.time() - start_time})\n\n    torch.save(model.generator.state_dict(), './dcgan_generator.pth')\n\n    torch.save(model.discriminator.state_dict(), './dcgan_discriminator.pth')\n\n    wandb.finish()\n\n \n\n# \u4e3b\u7a0b\u5e8f\u5165\u53e3\n\nif __name__ == \"__main__\":\n\n    dataloader = get_dataloader()\n\n    dcgan_model = DCGAN(nz=100, ngf=64, ndf=64, nc=3)\n\n    train_dcgan(dcgan_model, dataloader)",
        "time": 1860
    },
    {
        "code": "import torch\n\nimport torchvision\n\nimport torchvision.transforms as transforms\n\nimport torch.optim as optim\n\nimport torch.nn as nn\n\nimport timm\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"gpu-performance-benchmark\",name=\"ViT-style1\")\n\n \n\n# \u5b9a\u4e49\u6570\u636e\u9884\u5904\u7406\n\ndef get_transform():\n\n    return transforms.Compose([\n\n        transforms.Resize(256),\n\n        transforms.CenterCrop(224),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n    ])\n\n \n\n# \u6570\u636e\u96c6\u52a0\u8f7d\u51fd\u6570\n\ndef load_datasets(transform):\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n    return trainset, testset\n\n \n\n# \u6570\u636e\u52a0\u8f7d\u5668\u51fd\u6570\n\ndef get_dataloaders(trainset, testset, batch_size=32, num_workers=2):\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n    return trainloader, testloader\n\n \n\n# \u6a21\u578b\u5b9a\u4e49\n\nclass VisionTransformer(nn.Module):\n\n    def __init__(self, model_name='vit_base_patch16_224', num_classes=10, dropout_rate=0.5):\n\n        super(VisionTransformer, self).__init__()\n\n        self.model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n\n        self.dropout = nn.Dropout(dropout_rate)\n\n \n\n    def forward(self, x):\n\n        x = self.model(x)\n\n        x = self.dropout(x)\n\n        return x\n\n \n\n# \u8bbe\u7f6e\u8bbe\u5907\n\ndef get_device():\n\n    return torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n\n \n\n# \u8bad\u7ec3\u4e00\u4e2aepoch\n\ndef train_one_epoch(model, dataloader, criterion, optimizer, device):\n\n    model.train()\n\n    epoch_loss = 0.0\n\n    for inputs, labels in dataloader:\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n \n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n\n        optimizer.step()\n\n \n\n        epoch_loss += loss.item()\n\n \n\n    return epoch_loss / len(dataloader)\n\n \n\n# \u9a8c\u8bc1\u6a21\u578b\n\ndef evaluate_model(model, dataloader, criterion, device):\n\n    model.eval()\n\n    val_loss = 0.0\n\n    correct_predictions = 0\n\n    total_samples = 0\n\n \n\n    with torch.no_grad():\n\n        for inputs, labels in dataloader:\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n \n\n            _, predictions = torch.max(outputs.data, 1)\n\n            correct_predictions += (predictions == labels).sum().item()\n\n            total_samples += labels.size(0)\n\n \n\n    avg_val_loss = val_loss / len(dataloader)\n\n    accuracy = 100 * correct_predictions / total_samples\n\n \n\n    return avg_val_loss, accuracy\n\n \n\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u6d41\u7a0b\n\ndef train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, device):\n\n    start_time = time.time()\n\n \n\n    for epoch in range(num_epochs):\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n\n \n\n        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n\n        val_loss, val_accuracy = evaluate_model(model, test_loader, criterion, device)\n\n \n\n        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n\n       \n\n        wandb.log({\n\n            \"epoch\": epoch + 1,\n\n            \"train_loss\": train_loss,\n\n            \"val_loss\": val_loss,\n\n            \"val_accuracy\": val_accuracy\n\n        })\n\n \n\n        scheduler.step()\n\n \n\n    end_time = time.time()\n\n    training_time = end_time - start_time\n\n    print(f'Training Time: {training_time:.2f} seconds')\n\n \n\n    wandb.log({\"training_time\": training_time})\n\n    wandb.finish()\n\n \n\n# \u4e3b\u7a0b\u5e8f\n\ndef main():\n\n    transform = get_transform()\n\n    trainset, testset = load_datasets(transform)\n\n    train_loader, test_loader = get_dataloaders(trainset, testset)\n\n \n\n    device = get_device()\n\n    model = VisionTransformer().to(device)\n\n \n\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n \n\n    train_and_evaluate(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs=80, device=device)\n\n \n\n    model_path = './vit_cifar10.pth'\n\n    torch.save(model.state_dict(), model_path)\n\n \n\nif __name__ == \"__main__\":\n\n    main()",
        "time": 30624
    },
    {
        "code": "import torch\n\nimport torchvision\n\nimport torchvision.transforms as transforms\n\nimport torch.optim as optim\n\nimport torch.nn as nn\n\nimport timm\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"gpu-performance-benchmark\",name=\"ViT-style2\")\n\n \n\n# \u914d\u7f6e\u53c2\u6570\n\nconfig = {\n\n    \"batch_size\": 32,\n\n    \"num_workers\": 2,\n\n    \"learning_rate\": 0.001,\n\n    \"num_epochs\": 80,\n\n    \"model_name\": 'vit_base_patch16_224',\n\n    \"num_classes\": 10,\n\n    \"dropout_rate\": 0.5,\n\n    \"scheduler_step_size\": 10,\n\n    \"scheduler_gamma\": 0.1,\n\n    \"device\": torch.device(\"cuda:6\" if torch.cuda.is_available() else \"cpu\")\n\n}\n\n \n\n# \u6570\u636e\u9884\u5904\u7406\u548c\u52a0\u8f7d\n\ndef prepare_data(batch_size, num_workers):\n\n    transform = transforms.Compose([\n\n        transforms.Resize(256),\n\n        transforms.CenterCrop(224),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n    ])\n\n   \n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n   \n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n   \n\n    return trainloader, testloader\n\n \n\n# \u6a21\u578b\u5b9a\u4e49\n\ndef build_model(model_name, num_classes, dropout_rate, device):\n\n    model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)\n\n    model.dropout = nn.Dropout(dropout_rate)\n\n    return model.to(device)\n\n \n\n# \u8bad\u7ec3\u51fd\u6570\n\ndef run_training(trainloader, model, criterion, optimizer, scheduler, device, num_epochs):\n\n    train_loss = 0.0\n\n    start_time = time.time()  # \u5f00\u59cb\u65f6\u95f4\u8bb0\u5f55\n\n \n\n    for epoch in range(num_epochs):\n\n        model.train()\n\n        epoch_start_time = time.time()\n\n \n\n        for inputs, labels in trainloader:\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n \n\n            optimizer.zero_grad()\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n            loss.backward()\n\n            optimizer.step()\n\n \n\n            train_loss += loss.item()\n\n \n\n        scheduler.step()\n\n \n\n        elapsed_time = time.time() - epoch_start_time\n\n        avg_train_loss = train_loss / len(trainloader)\n\n \n\n        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Time: {elapsed_time:.2f}s')\n\n        wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss, \"epoch_time\": elapsed_time})\n\n \n\n    total_training_time = time.time() - start_time  # \u603b\u65f6\u95f4\u8bb0\u5f55\n\n    wandb.log({\"total_training_time\": total_training_time})\n\n    print(f'Total Training Time: {total_training_time:.2f} seconds')\n\n \n\n# \u9a8c\u8bc1\u51fd\u6570\n\ndef run_evaluation(testloader, model, criterion, device):\n\n    model.eval()\n\n    correct = 0\n\n    total = 0\n\n    val_loss = 0.0\n\n   \n\n    with torch.no_grad():\n\n        for inputs, labels in testloader:\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n           \n\n            _, predicted = torch.max(outputs, 1)\n\n            correct += (predicted == labels).sum().item()\n\n            total += labels.size(0)\n\n   \n\n    avg_val_loss = val_loss / len(testloader)\n\n    accuracy = 100 * correct / total\n\n    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n\n    wandb.log({\"val_loss\": avg_val_loss, \"val_accuracy\": accuracy})\n\n \n\n# \u4e3b\u7a0b\u5e8f\n\ndef main(config):\n\n    trainloader, testloader = prepare_data(config[\"batch_size\"], config[\"num_workers\"])\n\n    model = build_model(config[\"model_name\"], config[\"num_classes\"], config[\"dropout_rate\"], config[\"device\"])\n\n   \n\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"scheduler_step_size\"], gamma=config[\"scheduler_gamma\"])\n\n   \n\n    run_training(trainloader, model, criterion, optimizer, scheduler, config[\"device\"], config[\"num_epochs\"])\n\n    run_evaluation(testloader, model, criterion, config[\"device\"])\n\n \n\n    torch.save(model.state_dict(), './vit_cifar10_final.pth')\n\n    wandb.finish()\n\n \n\nif __name__ == \"__main__\":\n\n    main(config)",
        "time": 31180
    },
    {
        "code": "import torch\n\nfrom torch.utils.data import DataLoader\n\nfrom transformers import BertForSequenceClassification, AdamW\n\nimport wandb\n\nimport time\n\n \n\nwandb.init(project=\"sentiment analysis\",name=\"bert_style1\")\n\n \n\n# \u6570\u636e\u96c6\u52a0\u8f7d\n\ntrain_dataset = torch.load('data/train_dataset.pt')\n\ntest_dataset = torch.load('data/test_dataset.pt')\n\n \n\n# \u6a21\u578b\u521d\u59cb\u5316\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n \n\n# \u8d85\u53c2\u6570\u8bbe\u7f6e\n\nbatch_size = 64\n\nlearning_rate = 3e-5\n\nepochs = 75\n\n \n\n# \u6570\u636e\u52a0\u8f7d\u5668\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\n \n\n# \u4f18\u5316\u5668\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n \n\n# \u8bbe\u5907\u8bbe\u7f6e\n\ndevice = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\n \n\n# \u8bad\u7ec3\u51fd\u6570\n\ndef train(model, dataloader, optimizer):\n\n    model.train()\n\n    total_loss = 0.0\n\n    for batch_idx, batch in enumerate(dataloader):\n\n        optimizer.zero_grad()\n\n        inputs = {k: v.to(device) for k, v in batch.items()}\n\n        outputs = model(**inputs)\n\n        loss = outputs.loss\n\n        loss.backward()\n\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        if batch_idx % 10 == 0:\n\n            print(f\"Batch {batch_idx}, Loss: {loss.item()}\")\n\n    return total_loss / len(dataloader)\n\n \n\n# \u9a8c\u8bc1\u51fd\u6570\n\ndef evaluate(model, dataloader):\n\n    model.eval()\n\n    correct_predictions = 0\n\n    total_predictions = 0\n\n    total_loss = 0.0\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    with torch.no_grad():\n\n        for batch_idx, batch in enumerate(dataloader):\n\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n\n            labels = batch['labels'].to(device)\n\n            outputs = model(**inputs)\n\n            loss = criterion(outputs.logits, labels)\n\n            total_loss += loss.item()\n\n            predictions = torch.argmax(outputs.logits, dim=-1)\n\n            correct_predictions += (predictions == labels).sum().item()\n\n            total_predictions += labels.size(0)\n\n            if batch_idx % 100 == 0:\n\n                print(f\"Batch {batch_idx}, Evaluation Loss: {loss.item()}\")\n\n    accuracy = correct_predictions / total_predictions\n\n    return accuracy, total_loss / len(dataloader)\n\n \n\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u5faa\u73af\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n\n    print(f\"Epoch {epoch+1}/{epochs}\")\n\n    avg_train_loss = train(model, train_loader, optimizer)\n\n    val_accuracy, avg_val_loss = evaluate(model, test_loader)\n\n    print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}')\n\n   \n\n    wandb.log({\n\n        \"epoch\": epoch + 1,\n\n        \"train_loss\": avg_train_loss,\n\n        \"val_accuracy\": val_accuracy,\n\n        \"val_loss\": avg_val_loss\n\n    })\n\n \n\nend_time = time.time()\n\ntotal_training_time = end_time - start_time\nprint(f'Training Time: {total_training_time:.2f} seconds')\n\n \n\nwandb.log({\"training_time\": total_training_time})\nwandb.finish()",
        "time": 48147
    },
    {
        "code": "import torch\n\nfrom torch.utils.data import DataLoader\n\nfrom transformers import BertForSequenceClassification, AdamW\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316wandb\n\nwandb.init(project=\"sentiment analysis\",name=\"bert_style2\")\n\n \n\n# \u6570\u636e\u96c6\u52a0\u8f7d\n\ndef load_datasets(train_path, test_path):\n\n    return torch.load(train_path), torch.load(test_path)\n\n \n\ntrain_dataset, test_dataset = load_datasets('data/train_dataset.pt', 'data/test_dataset.pt')\n\n \n\n# \u6a21\u578b\u521d\u59cb\u5316\n\ndef initialize_model(pretrained_model_name, num_labels):\n\n    model = BertForSequenceClassification.from_pretrained(pretrained_model_name, num_labels=num_labels)\n\n    return model\n\n \n\nmodel = initialize_model('bert-base-uncased', num_labels=2)\n\n \n\n# \u8bbe\u7f6e\u53c2\u6570\n\nconfig = {\n\n    \"batch_size\": 64,\n\n    \"learning_rate\": 3e-5,\n\n    \"epochs\": 75\n\n}\n\n \n\n# \u6570\u636e\u52a0\u8f7d\u5668\n\ntrain_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n\n \n\n# \u4f18\u5316\u5668\n\noptimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n\n \n\n# \u8bbe\u7f6e\u8bbe\u5907\n\ndevice = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\n \n\n# \u8bad\u7ec3\u51fd\u6570\n\ndef train_one_epoch(model, dataloader, optimizer, device):\n\n    model.train()\n\n    cumulative_loss = 0.0\n\n    for batch_index, batch in enumerate(dataloader):\n\n        optimizer.zero_grad()\n\n        input_data = {key: value.to(device) for key, value in batch.items()}\n\n        output = model(**input_data)\n\n        loss = output.loss\n\n        loss.backward()\n\n        optimizer.step()\n\n        cumulative_loss += loss.item()\n\n        if batch_index % 10 == 0:\n\n            print(f\"Batch {batch_index}, Loss: {loss.item()}\")\n\n    return cumulative_loss / len(dataloader)\n\n \n\n# \u9a8c\u8bc1\u51fd\u6570\n\ndef evaluate_model(model, dataloader, device):\n\n    model.eval()\n\n    total_correct = 0\n\n    total_samples = 0\n\n    total_loss = 0.0\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n   \n\n    with torch.no_grad():\n\n        for batch_index, batch in enumerate(dataloader):\n\n            inputs = {key: value.to(device) for key, value in batch.items() if key != 'labels'}\n\n            labels = batch['labels'].to(device)\n\n            output = model(**inputs)\n\n            loss = criterion(output.logits, labels)\n\n            total_loss += loss.item()\n\n            predictions = torch.argmax(output.logits, dim=-1)\n\n            total_correct += (predictions == labels).sum().item()\n\n            total_samples += labels.size(0)\n\n            if batch_index % 100 == 0:\n\n                print(f\"Batch {batch_index}, Evaluation Loss: {loss.item()}\")\n\n   \n\n    accuracy = total_correct / total_samples\n\n    avg_loss = total_loss / len(dataloader)\n\n    return accuracy, avg_loss\n\n \n\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u5faa\u73af\n\ndef run_training(model, train_loader, test_loader, optimizer, config, device):\n\n    start_time = time.time()\n\n    for epoch in range(config[\"epochs\"]):\n\n        print(f\"Epoch {epoch + 1}/{config['epochs']}\")\n\n        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n\n        val_accuracy, val_loss = evaluate_model(model, test_loader, device)\n\n        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}, Validation Loss: {val_loss:.4f}')\n\n       \n\n        wandb.log({\n\n            \"epoch\": epoch + 1,\n\n            \"train_loss\": train_loss,\n\n            \"val_accuracy\": val_accuracy,\n\n            \"val_loss\": val_loss\n\n        })\n\n   \n\n    end_time = time.time()\n\n    total_training_time = end_time - start_time\n\n    print(f'Training Time: {total_training_time:.2f} seconds')\n\n   \n\n    wandb.log({\"training_time\": total_training_time})\n\n    wandb.finish()\n\n \n\n# \u6267\u884c\u8bad\u7ec3\n\nrun_training(model, train_loader, test_loader, optimizer, config, device)",
        "time": 47899
    },
    {
        "code": "import torch\n\nimport torchvision\n\nimport torchvision.transforms as transforms\n\nimport torch.optim as optim\n\nimport torch.nn as nn\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"cifar100-resnet50\",name=\"resnet50-style1\")\n\n \n\n# \u914d\u7f6e\u53c2\u6570\n\nconfig = {\n\n    \"batch_size\": 32,\n\n    \"num_workers\": 2,\n\n    \"learning_rate\": 0.003,\n\n    \"num_epochs\": 70,\n\n    \"step_size\": 10,\n\n    \"gamma\": 0.1,\n\n    \"device\": torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\"),\n\n}\n\n \n\n# \u6570\u636e\u9884\u5904\u7406\n\ndef get_transforms(train=True):\n\n    if train:\n\n        return transforms.Compose([\n\n            transforms.RandomResizedCrop(224),\n\n            transforms.RandomHorizontalFlip(),\n\n            transforms.ToTensor(),\n\n            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n\n        ])\n\n    else:\n\n        return transforms.Compose([\n\n            transforms.Resize(256),\n\n            transforms.CenterCrop(224),\n\n            transforms.ToTensor(),\n\n            transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n\n        ])\n\n \n\n# \u6570\u636e\u52a0\u8f7d\n\ndef prepare_dataloaders(config):\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=get_transforms(True))\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=get_transforms(False))\n\n   \n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n\n   \n\n    return trainloader, testloader\n\n \n\n# \u6a21\u578b\u521d\u59cb\u5316\n\ndef initialize_model(device):\n\n    model = torchvision.models.resnet50(pretrained=True)\n\n    model.fc = nn.Linear(model.fc.in_features, 100)\n\n    return model.to(device)\n\n \n\n# \u8bad\u7ec3\u4e00\u4e2a epoch\n\ndef train_one_epoch(model, trainloader, criterion, optimizer, device):\n\n    model.train()\n\n    running_loss = 0.0\n\n    for i, data in enumerate(trainloader):\n\n        inputs, labels = data\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n \n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n\n        optimizer.step()\n\n \n\n        running_loss += loss.item()\n\n        if (i + 1) % 200 == 0:\n\n            avg_loss = running_loss / 200\n\n            print(f'Batch {i + 1}, Loss: {avg_loss:.3f}')\n\n            running_loss = 0.0\n\n            wandb.log({\"batch\": i + 1, \"loss\": avg_loss})\n\n \n\n# \u6a21\u578b\u9a8c\u8bc1\n\ndef evaluate(model, testloader, criterion, device):\n\n    model.eval()\n\n    val_loss = 0.0\n\n    correct = 0\n\n    total = 0\n\n    with torch.no_grad():\n\n        for data in testloader:\n\n            images, labels = data\n\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = model(images)\n\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n\n            total += labels.size(0)\n\n            correct += (predicted == labels).sum().item()\n\n \n\n    val_loss /= len(testloader)\n\n    accuracy = 100 * correct / total\n\n    print(f'Validation Loss: {val_loss:.3f}, Validation Accuracy: {accuracy:.2f}%')\n\n    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": accuracy})\n\n \n\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\n\ndef train_and_evaluate(config):\n\n    trainloader, testloader = prepare_dataloaders(config)\n\n    model = initialize_model(config[\"device\"])\n\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=config[\"step_size\"], gamma=config[\"gamma\"])\n\n \n\n    start_time = time.time()\n\n \n\n    for epoch in range(config[\"num_epochs\"]):\n\n        print(f'Epoch {epoch + 1}/{config[\"num_epochs\"]}')\n\n        train_one_epoch(model, trainloader, criterion, optimizer, config[\"device\"])\n\n        evaluate(model, testloader, criterion, config[\"device\"])\n\n        scheduler.step()\n\n \n\n    training_time = time.time() - start_time\n\n    print(f'Training Time: {training_time:.2f} seconds')\n\n    wandb.log({\"training_time\": training_time})\n\n \n\n    torch.save(model.state_dict(), './resnet50_cifar100.pth')\n\n    wandb.finish()\n\n \n\n# \u4e3b\u7a0b\u5e8f\u5165\u53e3\n\nif __name__ == \"__main__\":\n\n    train_and_evaluate(config)",
        "time": 8880
    },
    {
        "code": "import torch\n\nimport torchvision\n\nimport torchvision.transforms as transforms\n\nimport torch.optim as optim\n\nimport torch.nn as nn\n\nimport wandb\n\nimport time\n\n \n\n# \u521d\u59cb\u5316 wandb\n\nwandb.init(project=\"cifar100-resnet50\",name=\"resnet50-style2\")\n\n \n\n# \u6570\u636e\u9884\u5904\u7406\n\ntrain_transform = transforms.Compose([\n\n    transforms.RandomResizedCrop(224),\n\n    transforms.RandomHorizontalFlip(),\n\n    transforms.ToTensor(),\n\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n\n])\n\n \n\ntest_transform = transforms.Compose([\n\n    transforms.Resize(256),\n\n    transforms.CenterCrop(224),\n\n    transforms.ToTensor(),\n\n    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]),\n\n])\n\n \n\n# \u6570\u636e\u52a0\u8f7d\n\ndef load_data(batch_size, num_workers):\n\n    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n\n    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n\n   \n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n   \n\n    return trainloader, testloader\n\n \n\n# \u6a21\u578b\u5b9a\u4e49\n\ndef create_model(num_classes):\n\n    model = torchvision.models.resnet50(pretrained=True)\n\n    model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n    return model\n\n \n\n# \u8bad\u7ec3\u8fc7\u7a0b\n\ndef train(model, trainloader, criterion, optimizer, device):\n\n    model.train()\n\n    total_loss = 0.0\n\n    for inputs, labels in trainloader:\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n       \n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n\n        optimizer.step()\n\n       \n\n        total_loss += loss.item()\n\n   \n\n    avg_loss = total_loss / len(trainloader)\n\n    return avg_loss\n\n \n\n# \u9a8c\u8bc1\u8fc7\u7a0b\n\ndef validate(model, testloader, criterion, device):\n\n    model.eval()\n\n    val_loss = 0.0\n\n    correct = 0\n\n    total = 0\n\n   \n\n    with torch.no_grad():\n\n        for inputs, labels in testloader:\n\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n\n           \n\n            _, predicted = torch.max(outputs, 1)\n\n            correct += (predicted == labels).sum().item()\n\n            total += labels.size(0)\n\n   \n\n    avg_val_loss = val_loss / len(testloader)\n\n    accuracy = 100 * correct / total\n\n    return avg_val_loss, accuracy\n\n \n\n# \u8bad\u7ec3\u548c\u9a8c\u8bc1\u5faa\u73af\n\ndef train_and_validate(model, trainloader, testloader, criterion, optimizer, scheduler, num_epochs, device):\n\n    for epoch in range(num_epochs):\n\n        start_time = time.time()\n\n       \n\n        train_loss = train(model, trainloader, criterion, optimizer, device)\n\n        val_loss, val_accuracy = validate(model, testloader, criterion, device)\n\n       \n\n        print(f\"Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n\n       \n\n        wandb.log({\n\n            \"epoch\": epoch + 1,\n\n            \"train_loss\": train_loss,\n\n            \"val_loss\": val_loss,\n\n            \"val_accuracy\": val_accuracy\n\n        })\n\n       \n\n        scheduler.step()\n\n       \n\n        elapsed_time = time.time() - start_time\n\n        print(f\"Time: {elapsed_time:.2f}s\")\n\n \n\n# \u4e3b\u7a0b\u5e8f\n\ndef main():\n\n    batch_size = 32\n\n    num_workers = 2\n\n    num_epochs = 70\n\n    learning_rate = 0.001\n\n   \n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n   \n\n    trainloader, testloader = load_data(batch_size, num_workers)\n\n    model = create_model(num_classes=100).to(device)\n\n   \n\n    criterion = nn.CrossEntropyLoss()\n\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n\n   \n\n    # \u8bb0\u5f55\u8bad\u7ec3\u5f00\u59cb\u65f6\u95f4\n\n    start_time = time.time()\n\n   \n\n    train_and_validate(model, trainloader, testloader, criterion, optimizer, scheduler, num_epochs, device)\n\n   \n\n    # \u8ba1\u7b97\u5e76\u8bb0\u5f55\u603b\u8bad\u7ec3\u65f6\u95f4\n\n    total_time = time.time() - start_time\n\n    print(f\"Total Training Time: {total_time:.2f} seconds\")\n\n    wandb.log({\"training_time\": total_time})\n\n   \n\n    torch.save(model.state_dict(), './resnet50_cifar100.pth')\n\n    wandb.finish()\n\n \n\nif __name__ == \"__main__\":\n\n    main()",
        "time": 8838
    }
]